{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Vishnu Ram Murali\n",
    "    - Email: vrm32@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Jeromey Abraham\n",
    "    - Email: jja99@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: Mohamed Shehaf Aakil Sharfudeen\n",
    "    - Email: ms5475@drexel.edu\n",
    "- Group member 4\n",
    "    - Name: Jibin Joby\n",
    "    - Email: jj3225@drexel.edu\n",
    "\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 1: Textual feature extraction and numerical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module B _(35 points)_ Key word in context\n",
    "\n",
    "Key word in context (KWiC) is a common format for concordance lines, i.e., contextualized instances of principal words used in a book. More generally, KWiC is essentially the concept behind the utility of 'find in page' on document viewers and web browsers. This module builds up a KWiC utility for finding key word-containing sentences, and 'most relevant' paragraphs, quickly.\n",
    "\n",
    "__B1.__ _(3 points)_ Start by writing a function called `load_book` that reads in a book based on a provided `book_id` string and returns a list of `paragraphs` from the book. When book data is loaded, you should remove the space characters at the beginning and end of the text (e.g., using `strip()`). Then, to split books into paragraphs, use the `re.split()` method to split the input in cases where there are two or more new lines. Note, that books are in the provided `data/books` directory.\n",
    "\n",
    "Note: this module is not focused on text pre-processing beyond a split into paragraphs; you do _not_ need to remove markup or non-substantive content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1:Function(3/3)\n",
    "\n",
    "import re\n",
    "\n",
    "def load_book(book_id):\n",
    "    \n",
    "    #---Your code start here---\n",
    "    with open(f'data/books/{book_id}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text.strip()\n",
    "        paragraphs = re.split('\\n\\n+', text)\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: setuptools in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/vrm6007/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function, lets apply it to look at a few paragraphs from book 84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723\n",
      "These reflections have dispelled the agitation with which I began my\n",
      "letter, and I feel my heart glow with an enthusiasm which elevates me\n",
      "to heaven, for nothing contributes so much to tranquillize the mind as\n",
      "a steady purpose--a point on which the soul may fix its intellectual\n",
      "eye.  This expedition has been the favourite dream of my early years. I\n",
      "have read with ardour the accounts of the various voyages which have\n",
      "been made in the prospect of arriving at the North Pacific Ocean\n",
      "through the seas which surround the pole.  You may remember that a\n",
      "history of all the voyages made for purposes of discovery composed the\n",
      "whole of our good Uncle Thomas' library.  My education was neglected,\n",
      "yet I was passionately fond of reading.  These volumes were my study\n",
      "day and night, and my familiarity with them increased that regret which\n",
      "I had felt, as a child, on learning that my father's dying injunction\n",
      "had forbidden my uncle to allow me to embark in a seafaring life.\n"
     ]
    }
   ],
   "source": [
    "# B1:SanityCheck\n",
    "paragraphs = load_book('84')\n",
    "print(len(paragraphs))\n",
    "print(paragraphs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B2.__ _(10 points)_ Next, write a function called `kwic(paragraphs, search_terms)` that accepts a list of string `paragraphs` and a set of `search_term` strings. The function should:\n",
    "\n",
    "1. initialize `data` as a `defaultdict` of lists\n",
    "2. loop over the `paragraphs` and apply `spacy`'s processing to produce a `doc` for each;\n",
    "3. loop over the `doc.sents` resulting from each `paragraph`;\n",
    "4. loop over the words in each `sentence`;\n",
    "5. check: `if` a `word` is `in` the `search_terms` set;\n",
    "6. `if` (5), then `.append()` the reference under `data[word]` as a list: `[[i, j, k], sentence]`, where `i`, `j`, and `k` refer to the paragraph-in-book, sentence-in-paragraph, and word-in-sentence indices, respectively.\n",
    "\n",
    "Your output, `data`, should then be a default dictionary of lists of the format:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```\n",
    "\n",
    "Note, we have imported spacy and set it up to use the `\"en\"` model. This will require you to install spacy by running `pip install spacy` and downloading the `\"en\"` model by running the command `python -m spacy download en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2:Function(10/10)\n",
    "\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def kwic(paragraphs, search_terms = {}):\n",
    "    \n",
    "    #---Your code starts here---\n",
    "    data = defaultdict(list)\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        doc = nlp(paragraph)\n",
    "        for j, sent in enumerate(doc.sents):\n",
    "            for k, word in enumerate(sent):\n",
    "                if word.text in search_terms:\n",
    "                    data[word.text].append([[i, j, k], sent.text])\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your function using the paragraphs from your `load_book` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Ocean': [[[10, 2, 26],\n",
       "               'I\\nhave read with ardour the accounts of the various voyages which have\\nbeen made in the prospect of arriving at the North Pacific Ocean\\nthrough the seas which surround the pole.  ']],\n",
       "             'seas': [[[10, 2, 30],\n",
       "               'I\\nhave read with ardour the accounts of the various voyages which have\\nbeen made in the prospect of arriving at the North Pacific Ocean\\nthrough the seas which surround the pole.  '],\n",
       "              [[21, 8, 10],\n",
       "               'Shall I meet you again, after having traversed immense seas, and\\nreturned by the most southern cape of Africa or America?  '],\n",
       "              [[30, 2, 14],\n",
       "               'Thus far I have\\ngone, tracing a secure way over the pathless seas, the very stars\\nthemselves being witnesses and testimonies of my triumph.  '],\n",
       "              [[378, 4, 21],\n",
       "               'I had a very\\nconfused knowledge of kingdoms, wide extents of country, mighty rivers,\\nand boundless seas.  '],\n",
       "              [[668, 8, 20],\n",
       "               'I had determined, if you were going southwards, still to trust\\nmyself to the mercy of the seas rather than abandon my purpose.  '],\n",
       "              [[676, 15, 5],\n",
       "               'Behold, on these desert seas I have\\nfound such a one, but I fear I have gained him only to know his value\\nand lose him.  ']]})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B2:SanityCheck\n",
    "kwic(paragraphs, {'Ocean', 'seas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ _(2 points)_ Let's test your `kwic` search function's utility using the pre-processed `paragraphs` from book `84` for the key words `Frankenstein` and `monster` in context. Answer the inline questions about these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 27\n",
      "# of sentences 'monster' appears in: 31\n",
      "\n",
      "S h e   n u r s e d   M a d a m e \n",
      " F r a n k e n s t e i n ,   m y   a u n t ,   i n   h e r   l a s t   i l l n e s s ,   w i t h   t h e   g r e a t e s t   a f f e c t i o n \n",
      " a n d   c a r e   a n d   a f t e r w a r d s   a t t e n d e d   h e r   o w n   m o t h e r   d u r i n g   a   t e d i o u s \n",
      " i l l n e s s ,   i n   a   m a n n e r   t h a t   e x c i t e d   t h e   a d m i r a t i o n   o f   a l l   w h o   k n e w   h e r , \n",
      " a f t e r   w h i c h   s h e   a g a i n   l i v e d   i n   m y   u n c l e ' s   h o u s e ,   w h e r e   s h e   w a s   b e l o v e d \n",
      " b y   a l l   t h e   f a m i l y .    \n",
      "\n",
      "I   s t a r t e d   f r o m   m y   s l e e p   w i t h   h o r r o r ;   a   c o l d   d e w   c o v e r e d   m y   f o r e h e a d ,   m y \n",
      " t e e t h   c h a t t e r e d ,   a n d   e v e r y   l i m b   b e c a m e   c o n v u l s e d ;   w h e n ,   b y   t h e   d i m   a n d \n",
      " y e l l o w   l i g h t   o f   t h e   m o o n ,   a s   i t   f o r c e d   i t s   w a y   t h r o u g h   t h e   w i n d o w \n",
      " s h u t t e r s ,   I   b e h e l d   t h e   w r e t c h - - t h e   m i s e r a b l e   m o n s t e r   w h o m   I   h a d \n",
      " c r e a t e d .    \n"
     ]
    }
   ],
   "source": [
    "# B3:SanityCheck\n",
    "results = kwic(paragraphs, {\"Frankenstein\", \"monster\"})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(results['Frankenstein'][7][1]))\n",
    "print()\n",
    "print(\" \".join(results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow\n"
     ]
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# Is the kwic function fast or slow? Print \"Fast\" or \"Slow\"\n",
    "print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# How many sentences does the work Frankenstein appear in? Print the integer (0 is just a placeholder).\n",
    "print(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B4.__ _(10 pts)_ The cost of _indexing_ a given book turns out to be the limiting factor here for kwic. Presently, we have our pre-processing `load_book` function just splitting a document into paragraphs. Rewrite the `load_book` function to do some additional preprocessing. Specifically, this function should be modified to:\n",
    "\n",
    "1. split a `book` into paragraphs and loop over them, but\n",
    "2. process each paragraph with `spacy`;\n",
    "3. store the `document` as a triple-nested list, so that each word _string_ is reachable via three indices: `word = document[i][j][k]`;\n",
    "4. record an `index = defaultdict(list)` containing a list of `[i,j,k]` lists for each word; and\n",
    "5. `return document, index`\n",
    "\n",
    "Pre-computing the `index` will allow us to efficiently look up the locations of each word's instance in `document`, and the triple-list format of our document will allow us fast access to extract the sentence for KWiC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:Function(10/10)\n",
    "\n",
    "def load_book(book_id):\n",
    "    \n",
    "    #---Your code starts here---\n",
    "    with open(f\"data/books/{book_id}.txt\", \"r\") as f:\n",
    "        text = f.read().strip()\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    document = []\n",
    "    index = defaultdict(list)\n",
    "    for i in range(len(paragraphs)):\n",
    "        paragraph = paragraphs[i]\n",
    "        doc = nlp(paragraph)\n",
    "        sent_list = []\n",
    "        for j in range(len(list(doc.sents))):\n",
    "            sent = list(doc.sents)[j]\n",
    "            word_list = []\n",
    "            for k in range(len(sent)):\n",
    "                word = sent[k].text\n",
    "                word_list.append(word)\n",
    "                index[word].append([i, j, k])\n",
    "            sent_list.append(word_list)\n",
    "        document.append(sent_list)\n",
    "    #---Your code ends here---\n",
    "                    \n",
    "    return(document, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your new function on `book_id` = `'84'`. We'll use the returned document to access a particular sentence and print out the `[i,j,k]` locations of the word `'monster'` from `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# load the book\n",
    "document, index = load_book(\"84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " ',',\n",
       " 'Margaret',\n",
       " ',',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'is',\n",
       " 'forever',\n",
       " '\\n',\n",
       " 'visible',\n",
       " ',',\n",
       " 'its',\n",
       " 'broad',\n",
       " 'disk',\n",
       " 'just',\n",
       " 'skirting',\n",
       " 'the',\n",
       " 'horizon',\n",
       " 'and',\n",
       " 'diffusing',\n",
       " 'a',\n",
       " '\\n',\n",
       " 'perpetual',\n",
       " 'splendour',\n",
       " '.',\n",
       " ' ']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B4:SanityCHeck\n",
    "\n",
    "# Output paragraph 9, sentence 5\n",
    "document[9][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[124, 9, 57],\n",
       " [136, 3, 6],\n",
       " [139, 3, 4],\n",
       " [142, 1, 4],\n",
       " [243, 3, 29],\n",
       " [261, 3, 18],\n",
       " [280, 0, 2],\n",
       " [321, 1, 35],\n",
       " [345, 8, 6],\n",
       " [380, 12, 5],\n",
       " [397, 1, 46],\n",
       " [437, 1, 10],\n",
       " [439, 0, 3],\n",
       " [477, 6, 8],\n",
       " [478, 6, 6],\n",
       " [510, 1, 8],\n",
       " [527, 0, 1],\n",
       " [538, 18, 22],\n",
       " [560, 3, 31],\n",
       " [585, 3, 43],\n",
       " [587, 10, 72],\n",
       " [606, 3, 2],\n",
       " [615, 2, 11],\n",
       " [633, 8, 9],\n",
       " [639, 1, 21],\n",
       " [644, 4, 17],\n",
       " [653, 6, 5],\n",
       " [663, 4, 2],\n",
       " [673, 0, 39],\n",
       " [673, 1, 2],\n",
       " [709, 12, 1]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# Output the indices for monster\n",
    "index['monster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B5.__ _(5 pts)_ Finally, make a new function called `fast_kwic` that takes a `document` and `index` from our new `load_book` function as well as a provided list of `search_terms` (just like our original kwic function). The function should loops through all specified `search_terms` to identify indices from `index[word]` for the key word-containing sentences and use them to extract these sentences from `document` into the same data structure as output by __B2__:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B5:Function(5/5)\n",
    "\n",
    "def fast_kwic(document, index, search_terms = {}):\n",
    "\n",
    "    #---Your code starts here---\n",
    "    data = defaultdict(list)\n",
    "    \n",
    "    for search_term in search_terms:\n",
    "        if search_term in index:\n",
    "            for indices in index[search_term]:\n",
    "                i, j, k = indices\n",
    "                sentence = document[i][j]\n",
    "                if search_term not in data:\n",
    "                    data[search_term] = []\n",
    "                data[search_term].append([indices, sentence])\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our new function, lets test it on the same keywords as before: `Frankenstein` and `monster`. Note that the output from this sanity check should be the same as the one from **B3**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 27\n",
      "# of sentences 'monster' appears in: 31\n",
      "\n",
      "She nursed Madame \n",
      " Frankenstein , my aunt , in her last illness , with the greatest affection \n",
      " and care and afterwards attended her own mother during a tedious \n",
      " illness , in a manner that excited the admiration of all who knew her , \n",
      " after which she again lived in my uncle 's house , where she was beloved \n",
      " by all the family .  \n",
      "\n",
      "I started from my sleep with horror ; a cold dew covered my forehead , my \n",
      " teeth chattered , and every limb became convulsed ; when , by the dim and \n",
      " yellow light of the moon , as it forced its way through the window \n",
      " shutters , I beheld the wretch -- the miserable monster whom I had \n",
      " created .  \n"
     ]
    }
   ],
   "source": [
    "# B5:SanityCheck\n",
    "\n",
    "fast_results = fast_kwic(document, index, {'Frankenstein', 'monster'})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(fast_results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(fast_results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(fast_results['Frankenstein'][7][1]))\n",
    "print()\n",
    "print(\" \".join(fast_results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B6.__ _(5 pts)_ Your goal here is to modify the pre-processing in `load_book` one more time! Make a small modification to the input: `load_book(book_id, pos = True, lemma = True):`, to accept two boolean arguments, `pos` and `lemma` specifying how to identify each word as a key term. In particular, each word will now be represented in both of the `document` and `index` as a tuple: `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`.\n",
    "\n",
    "Note this functions output should still consist of a `document` and `index` in the same format aside from the replacement of `word` with `heading`, which will allow for the same use of output in `fast_kwic`, although more specified by the textual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:Function(5/5)\n",
    "\n",
    "def load_book(book_id, pos = True, lemma = True):\n",
    "    \n",
    "    #---Your code starts here---\n",
    "\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return document, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:SanityCheck\n",
    "document, index = load_book(\"84\", pos = True, lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with ('cold', 'NOUN'):\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# B6:SanityCheck\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence with (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcold\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOUN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mfast_kwic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_terms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNOUN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNOUN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'NOUN'):\")\n",
    "\" \".join(fast_kwic(document, index, search_terms = {('cold', 'NOUN')})[('cold', 'NOUN')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with ('cold', 'ADJ'):\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# B6:SanityCheck\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence with (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcold\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADJ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mfast_kwic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_terms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADJ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADJ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'ADJ'):\")\n",
    "\" \".join(fast_kwic(document, index, search_terms = {('cold', 'ADJ')})[('cold', 'ADJ')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
